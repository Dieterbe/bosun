tsdbHost = ny-tsdb03.ds.stackexchange.com:4242
relayListen = :4242
smtpHost = ny-mail.ds.stackexchange.com:25
emailFrom = bosun@stackexchange.com
httpListen = :80
timeAndDate = 202,75,179,136
ping = true
responseLimit = 5242880
squelch = host=jibson|-lab|ubuntu|-raspi|gbeech|tlim|pgrace|gb8|alear
squelch = pxname=_dev|^dev$|-dev

#Note(KMB)
# Warn vs Crit:
#   - Warn means we are seeing issues or resources issues that may indiciate a potential usre facing issue
#   - Crit means there are issues or resource issues that means that is likely a problem that is currently
#     causing issues for users 

###################
## Unknown
###################

template unknown {
    subject = {{.Name}}: {{.Group | len}} unknown alerts
    body = `
    <p>Time: {{.Time}}
    <p>Name: {{.Name}}
    <p>Alerts:
    {{range .Group}}
        <br>{{.}}
    {{end}}`
}

unknownTemplate = unknown

###################
## Templates
###################

###################
## Partials
###################

template def {
    body = `<p><strong>Alert definition:</strong>
    <pre>
{{.Alert.Def}}
</pre>
`
}

template computation {
    body = `
    <p><strong>Computation</strong>
    
    <table>
        {{range .Computations}}
            <tr><td><a href="{{$.Expr .Text}}">{{.Text}}</a></td><td>{{.Value}}</td></tr>
        {{end}}
    </table>`
}

template tags {
    body = `<p><strong>Tags</strong>
    
    <table>
        {{range $k, $v := .Group}}
            {{if eq $k "host"}}
                <tr><td>{{$k}}</td><td><a href="{{$.HostView $v}}">{{$v}}</a></td></tr>
            {{else}}
                <tr><td>{{$k}}</td><td>{{$v}}</td></tr>
            {{end}}
        {{end}}
    </table>`
}

template header {
    body = `<p><a href="{{.Ack}}">Acknowledge alert</a>
    <p><a href="{{.Rule}}">View the Rule + Template in the Bosun's Rule Page</a>
    {{if .Alert.Vars.notes}}
    <p>Notes: {{.Alert.Vars.notes}}
    {{end}}
    {{if .Group.host}}
    <p><a href="https://status.stackexchange.com/dashboard/node?node={{.Group.host}}">View Host {{.Group.host}} in Opserver</a>
    {{end}}
    `
}

###################
## Full
###################

template generic {
    body = `{{template "header" .}}
    {{template "def" .}}
    
    {{template "tags" .}}

    {{template "computation" .}}`
    subject = {{.Last.Status}}: {{replace .Alert.Name "." " " -1}}: {{.Eval .Alert.Vars.q | printf "%.2f"}}{{if .Alert.Vars.unit_string}}{{.Alert.Vars.unit_string}}{{end}} on {{.Group.host}}
}

template generic.it {
    body = `{{template "header" .}}
    {{template "def" .}}
    
    {{template "tags" .}}

    {{template "computation" .}}`
    subject = {{.Last.Status}}: {{replace .Alert.Name "." " " -1}}: {{.Eval .Alert.Vars.q}} on {{.Group.host}} (To Acknowledge: {{.Ack}})
}

template it.packetloss {
    body = `{{template "header" .}}
    {{template "def" .}}

    {{template "tags" .}}

    {{template "computation" .}}`
    subject = {{.Last.Status}}: {{replace .Alert.Name "." " " -1}}: {{.Eval .Alert.Vars.q}}% {{.Group.host}} --> {{.Group.destination}} (To Acknowledge: {{.Ack}})
}


template asterisk.call.quality
{
    body = `{{template "header" .}}
    {{template "def" .}}

    {{template "tags" .}}

    {{template "computation" .}}`
    subject = {{.Last.Status}}: {{replace .Alert.Name "." " " -1}}: {{.Group.sender}} -> {{.Group.receiver}} has A Mean Opinion Score/Listening Quality Objective of **{{.Eval .Alert.Vars.q}}** (To Acknowledge: {{.Ack}})
}

template generic.cw {
    body = `{{template "header" .}}

    {{template "def" .}}
    {{template "tags" .}}

    {{template "computation" .}}`
    subject = `{{.Last.Status}}: {{replace .Alert.Name "." " " -1}}: 
        {{if eq .Last.Status 1}} {{ .Eval .Alert.Vars.qc | printf "%.2f"}} {{ end }}
        {{if eq .Last.Status 2}} {{ .Eval .Alert.Vars.qw | printf "%.2f"}} {{ end }}
        {{if eq .Last.Status 3}} {{ .Eval .Alert.Vars.qc | printf "%.2f"}} {{ end }}
    on {{.Group.host}}`
}

template haproxy.server.downtime {
    body = `{{template "header" .}}
    <p><a href="https://status.stackexchange.com/haproxy">View HAProxy info in Opserver</a>
    <h2>Down HAProxy Servers By Host, Backend, and Server</h2>
    <table>
        <tr><th>Host</th><th>Backend</th><th>Server</th><th>Seconds Down in the Past {{.Alert.Vars.time}} {{.Alert.Vars.t}}</th></tr>
        {{range $r := .LeftJoin .Alert.Vars.downtime .Alert.Vars.maintenance}}
          {{ $d := index $r 0 }}
          {{ $m := index $r 1 }}
          {{ if and (gt $d.Value 120.0) (not (eq $m.Value 3.0))}}
            <tr>
              <td>{{ $d.Group.host}}</td>
              <td>{{ $d.Group.pxname}}</td>
              <td>{{ $d.Group.svname}}</td>
              <td {{if gt $d.Value 120.0}} style="color: red;" {{end}}>{{(index $r 0).Value | printf "%.f"}}</td>
            </tr>
          {{end}}
        {{end}}
    </table>
    <h2>Percentage of Down Servers By Host and Backend</h2>
    <table>
        <tr><th>Host</th><th>Backend</th><th>Percent of Down Servers</th></tr>
        {{range $r := .EvalAll .Alert.Vars.percent_down}}
        {{ if ne $r.Value 0.0 }}
            <tr>
              <td>{{$r.Group.host}}</td>
              <td>{{$r.Group.pxname}}</td>
              <td {{if gt $r.Value 50.0}} style="color: red;" {{end}}>{{$r.Value}}</td>
            </tr>
        {{end}}
        {{end}}
    </table>
`
    subject = `{{.Last.Status}}: 
            {{if .Last.Status.IsWarning}} At least on backend has {{.Eval .Alert.Vars.max_down_count}} servers down {{end}}
            {{if .Last.Status.IsCritical}} At least on backend has {{.Eval .Alert.Vars.max_percent_down | printf "%.2f"}} percent servers down {{end}}`
}

template redis.replication {
    body = `{{template "header" .}}
    <p><a href="https://status.stackexchange.com/redis">View Redis Information in Opserver</a>
    <table>
        <tr><th>Redis Port</th><th>Redis Replication Health</th><th>Slave Count with Good Master Link Status</th><th>Expected Slaves</th></tr>
    {{ range $r := ..LeftJoin .Alert.Vars.health_by_port .Alert.Vars.link_status_by_port}}
    {{ $health := index $r 0 }}
    {{ $total_links := index $r 1 }}
       <tr>
        <td>{{$.LookupAll "redis" "name" $health.Group}} ({{$health.Group.port}})</td>
        <td>{{ if gt $health.Value 0.0}}<span style="color: red;">Bad</span>{{else}}Good{{end}}</td>
        <td>{{$total_links.Value}}</td>
        <td>{{$.LookupAll "redis" "slave_count" $health.Group}}</td>
       </tr>
    {{ end }}  
   </table>
    {{ range $r := .EvalAll .Alert.Vars.health_by_port}}
      {{ if gt $r.Value 0.0}}
      <h2>{{$.LookupAll "redis" "name" $r.Group}} ({{$r.Group.port}})</h2>
      <table>
      <tr><th>Host</th><th>Is Slave</th><th>Master Sync in Progress</th><th>Sync Ago in Seconds</th></tr>
      {{ range $s := $.LeftJoin $.Alert.Vars.is_slave $.Alert.Vars.sync_status $.Alert.Vars.sync_ago $.Alert.Vars.connected_slaves}}
        {{if eq $r.Group.port ((index $s 0).Group.port)}}
          <tr>
            <td>{{ (index $s 0).Group.host }}</td>
            <td>{{ (index $s 0).Value }}</td>
            <td>{{ (index $s 1).Value }}</td>
            <td>{{ (index $s 2).Value }}</td>
            <td>{{ (index $s 3).Value }}</td>
          </tr>
        {{end}}
      {{ end }}
    {{ end }}
    </table>
  {{ end }}
    `
    subject = {{.Last.Status}}: Redis has {{.Alert.Vars.total_bad | .Eval}} instances with bad replication.  {{ range $r := .EvalAll .Alert.Vars.health_by_port}} {{if gt $r.Value 0.0}} {{$r.Group.port}}{{end}}{{end}}
}

template redis.fragmentation {
    body = `{{template "header" .}}
    <p><a href="https://status.stackexchange.com/redis">View Redis Information in Opserver</a>
    <p>Instance: {{.Lookup "redis" "name"}} [:{{.Group.port}}] on {{.Group.host}}
    <br>Redis Used: {{ .Alert.Vars.redis_used | .Eval | printf "%.2f" | bytes}}
    <br>Used RSS: {{ .Alert.Vars.used_rss | .Eval | printf "%.2f" | bytes}}
    <br>Fragmentation Ratio:  {{ .Alert.Vars.frag | .Eval | printf "%.2f"}}
    `
    subject = {{.Last.Status}}: redis fragmentation is {{.Alert.Vars.frag | .Eval | printf "%.2f"}} on {{.Lookup "redis" "name"}} [:{{.Group.port}}] on {{.Group.host}}
}

template diskspace {
    body = `{{template "header" .}}
    <p>Host: <a href="{{.HostView .Group.host | short }}">{{.Group.host}}</a>
    <br>Disk: {{.Group.disk}}

    <p>Percent Free: {{.Eval .Alert.Vars.percent_free | printf "%.2f"}}%
    <br>Free: {{.Eval .Alert.Vars.free | bytes}}
    <br>Used: {{.Eval .Alert.Vars.used | bytes}}
    <br>Total: {{.Eval .Alert.Vars.total | bytes}}
    <br>Est. {{.Eval .Alert.Vars.days_to_zero | printf "%.2f"}} days remain until 0% free space
    {{/* .Graph .Alert.Vars.percent_free_graph */}}
    {{/* What is below can be replaced by the below once https://github.com/bosun-monitor/bosun/issues/348 is fixed */}}
    {{printf "q(\"avg:1h-min:os.disk.fs.percent_free{host=%s,disk=%s}\", \"7d\", \"\")" .Group.host .Group.disk | .Graph}}
    `
    subject = {{.Last.Status}}: Diskspace: ({{.Alert.Vars.used | .Eval | bytes}}/{{.Alert.Vars.total | .Eval | bytes}}) {{.Alert.Vars.percent_free | .Eval | printf "%.2f"}}% Free on {{.Group.host}}:{{.Group.disk}} (Est. {{.Eval .Alert.Vars.days_to_zero | printf "%.2f"}} days remain)
}

template route.performance {
    body = `{{template "header" .}}
    <br>
    <br><span style="font-weight: bold;">Route: {{.Group.route}}</span>
    <br>Current Median: {{.Eval .Alert.Vars.current_median | printf "%.2f ms"}}
    <br>Past Median: {{.Eval .Alert.Vars.past_median | printf "%.2f ms"}}
    <br>Difference: {{.Eval .Alert.Vars.diff | printf "%.2f ms"}}
    <br>Route Hits: {{.Eval .Alert.Vars.route_hits | printf "%.2f hits"}}
    <br>Total Hits: {{.Eval .Alert.Vars.total_hits | printf "%.2f hits "}}
    <br>Route Hit Percentage of Total: {{.Eval .Alert.Vars.hit_percent | printf "%.2f"}}%
`
    subject = {{.Last.Status}}: Median Response Time Change of  {{.Eval .Alert.Vars.diff | printf "%.2f ms"}} (Current: {{.Eval .Alert.Vars.current_median | printf "%.2f ms"}} Past: {{.Eval .Alert.Vars.past_median | printf "%.2f ms"}}) on {{.Group.route}}
}

template exceptions {
    subject = {{.Last.Status}}: Exceptions In the past {{.Alert.Vars.t}} : {{.Eval .Alert.Vars.exceptions_total | printf "%.f" }}
    body = `{{template "header" .}}
        <h2>Overview</h2>
         <p>{{.Eval .Alert.Vars.exceptions_total | printf "%.f" }} Exceptions In the past {{.Alert.Vars.t}} exceeds treshold of {{.Alert.Vars.threshold}}
        <p>View Exceptions in <a href="https://status.stackexchange.com/exceptions">Opserver</a>
        <h2>Facets</h2>
        <table>
        <tr><th>Host</th><th>Total Excepts in the last {{.Alert.Vars.t}}</th></tr>
        {{range $r := .EvalAll .Alert.Vars.machine}}
            {{ if ne $r.Value 0.0 }}
                <tr><td>{{$r.Group.machine}}</td><td>{{$r.Value | printf "%.f"}}</td></tr>
            {{end}}
        {{end}}

        </br>
        <tr><th>Application</th><th></th></tr>
        {{range $r := .EvalAll .Alert.Vars.application}}
            {{ if ne $r.Value 0.0 }}
                <tr><td><a href="https://status.stackexchange.com/exceptions?log={{$r.Group.application}}">{{$r.Group.application}}</a></td><td>{{$r.Value | printf "%.f"}}</td></tr>
            {{end}}
        {{end}}
        </table>
        <h2>Exceptions Per Second Over The Last {{.Alert.Vars.graph_time}}</h2>
        <div>{{.Graph .Alert.Vars.exceptions_graph}}</div>
        <h3>By Server</h3>
        <div>{{.GraphAll .Alert.Vars.machine_graph}}</div>
        <h3>By App Pool</h3>
        <div>{{.GraphAll .Alert.Vars.application_graph}}</div>`
}

template puppet.failed_resources {
    body = `{{template "header" .}}
    <p>{{.Last.Status}}: {{ .Eval .Alert.Vars.q}} failed resources on {{.Group.host}}
    <h2>Resources</h2>
    <p><a href="http://puppet-dashboard.ds.stackexchange.com/nodes?q={{.Group.host}}">View {{.Group.host}} in Puppet Dashboard</a>
    <p><a href="http://puppetboard/node/{{.Group.host}}.ds.stackexchange.com">View {{.Group.host}} in PuppetBoard</a>
    `
    subject = `{{.Last.Status}}: {{ .Eval .Alert.Vars.q}} failed resources on {{.Group.host}}`
}

template bosun_build {
    body = `{{template "header" .}}
    <p>Bosun build failed for build {{.Group.build_type}}
    <p><a href="https://build.stackexchange.com/viewType.html?buildTypeId={{.Group.build_type}}">View Build history for {{.Group.build_type}}</a>
    `
    subject = {{.Last.Status}}: Bosun build failed for build {{.Group.build_type}}
}

template interface_drops {
    body = `{{template "header" .}}
    <h2>Average Drops,Errors, and Packets Per Second Per Interface</h2>
    <table>
        <tr><th>Interface</th><th>Label</th><th>Drops</th><th>Errors</th><tr>
        {{ range $v := .LeftJoin  .Alert.Vars.iface_drop .Alert.Vars.iface_err}}
            {{ $d := index $v 0 }}
                {{ $e := index $v 1 }}
                {{/* TODO Need to be able to Eval the threshold here */}}
                {{ if or (gt $d.Value  0.0) (gt $e.Value 0.0)}}
                    {{/* TODO Fix LeftJoin to the following if isn't needed */}}
                    {{ if ($d.Group.Subset $.Group) }}
                    <tr>
                            <td>{{$d.Group.iface}}</td>
                            <td>
                                {{ $.GetMeta "" "alias" $d.Group }}
                            </td>
                            <td>{{$d.Value | printf "%.2f"}}</td>
                            <td>{{$e.Value | printf "%.2f"}}</td>
                        </tr>
                    {{end}}
                {{end}}
        {{ end }}
    `
    subject = `{{.Last.Status}}: {{.Eval .Alert.Vars.exceed_count}} interfaces on {{.Group.host}} have dropped and/or erred packets `
}

template linux.tcp {
    body = `{{template "header" .}}
    <table>
        {{/* TODO: Reference what each stat means */}}
        <tr><th>Stat</th><th>Count in the last {{.Alert.Vars.time}}</th></tr>
        <tr><td>TCP Abort Failed</td><td>{{.Eval .Alert.Vars.abort_failed | printf "%.2f"}}<td></tr>
        <tr><td>Out Of Order Pruned</td><td>{{.Eval .Alert.Vars.ofo_pruned | printf "%.2f"}}<td></tr>
        <tr><td>Receive Pruned</td><td>{{.Eval .Alert.Vars.rcv_pruned | printf "%.2f"}}<td></tr>
        <tr><td>Backlog Dropped</td><td>{{.Eval .Alert.Vars.backlog_drop | printf "%.2f"}}<td></tr>
        <tr><td>Syn Cookies Sent</td><td>{{.Eval .Alert.Vars.syncookies_sent | printf "%.2f"}}<td></tr>
        <tr><td>TOTAL Of Above</td><td>{{.Eval .Alert.Vars.total_err | printf "%.2f"}}<td></tr>
    </table>
    `
    subject = `{{.Last.Status}}: {{.Eval .Alert.Vars.total_err | printf "%.2f"}} tcp errors on {{.Group.host}} `
}

template hardware {
    body = `
    <p>Overall System status is  {{if gt .Value 0.0}} <span style="color: red;">Bad</span>
              {{else}} <span style="color: green;">Ok</span>
              {{end}}</p>
    {{template "header" .}}
    <p><a href="https://{{.Group.host}}:1311/OMSALogin?msgStatus=null">OMSA Login for host {{.Group.host}}</a>
    <h3>General Host Info</h3>
    {{ with .GetMeta "" "svctag" (printf "host=%s" .Group.host) }}
        Service Tag: <a href="http://www.dell.com/support/home/us/en/04/product-support/servicetag/{{.}}">{{.}}</a>
    {{ end }}
    
    {{ with .GetMeta "" "model" (printf "host=%s" .Group.host) }}
        <br>Model: {{.}}
    {{ end }}
    
    {{ with .GetMeta "" "version" (printf "host=%s" .Group.host) }}
        <br>OS: {{.}}
    {{ end }}
    
    <h3>Power Supplies</h3>
    <table>
    <tr><th>Power Supply Id</th><th>Status</th></tr>
    {{range $r := .EvalAll .Alert.Vars.power}}
        {{if eq $r.Group.host $.Group.host}}
            <tr>
              <td>{{$r.Group.id}}</td>
              {{if gt $r.Value 0.0}} <td style="color: red;">Bad</td>
              {{else}} <td style="color: green;">Ok</td>
              {{end}}
            </tr>
        {{end}}
    {{end}}
    </table>
    
    <h3>Batteries</h3>
    <table>
    <tr><th>Battery Id</th><th>Status</th></tr>
    {{range $r := .EvalAll .Alert.Vars.battery}}
        {{if eq $r.Group.host $.Group.host}}
            <tr>
              <td>{{$r.Group.id}}</td>
            </tr>
        {{end}}
    {{end}}
    </table>
    
    <h3>Controllers</h3>
    <table>
    <tr><th>Controller Id</th><th>Status</th></tr>
    {{range $r := .EvalAll .Alert.Vars.controller}}
        {{if eq $r.Group.host $.Group.host}}
            <tr>
              <td>{{$r.Group.id}}</td>
              {{if gt $r.Value 0.0}} <td style="color: red;">Bad</td>
              {{else}} <td style="color: green;">Ok</td>
              {{end}}
            </tr>
        {{end}}
    {{end}}
    </table>

    <h3>Enclosures</h3>
    <table>
    <tr><th>Enclosure Id</th><th>Status</th></tr>
    {{range $r := .EvalAll .Alert.Vars.enclosure}}
      {{if eq $r.Group.host $.Group.host}}
            <tr>
              <td>{{$r.Group.id}}</td>
              {{if gt $r.Value 0.0}} <td style="color: red;">Bad</td>
              {{else}} <td style="color: green;">Ok</td>
              {{end}}
            </tr>
        {{end}}
    {{end}}
    </table>

    <h3>Physical Disks</h3>
    <table>
    <tr><th>Physical Disk Id</th><th>Status</th></tr>
    {{range $r := .EvalAll .Alert.Vars.physical_disk}}
      {{if eq $r.Group.host $.Group.host}}
            <tr>
              <td>{{$r.Group.id}}</td>
              {{if gt $r.Value 0.0}} <td style="color: red;">Bad</td>
              {{else}} <td style="color: green;">Ok</td>
              {{end}}
            </tr>
        {{end}}
    {{end}}
    </table>

    <h3>Virtual Disks</h3>
    <table>
    <tr><th>Virtual Disk Id</th><th>Status</th></tr>
    {{range $r := .EvalAll .Alert.Vars.virtual_disk}}
      {{if eq $r.Group.host $.Group.host}}
            <tr>
              <td>{{$r.Group.id}}</td>
              {{if gt $r.Value 0.0}} <td style="color: red;">Bad</td>
              {{else}} <td style="color: green;">Ok</td>
              {{end}}
            </tr>
        {{end}}
    {{end}}
    </table>
    `
    subject = {{.Last.Status}}: {{replace .Alert.Name "." " " -1}}: on {{.Group.host}}
}

template high.cpu {
    body = `{{template "header" .}}
    <h3>CPU Utilization Percentage (0-100%)</h3>
    <p>Current Median: {{.Eval .Alert.Vars.current_median | printf "%.2f"}}
    <br>Historical Difference (Current - Past): {{.Eval .Alert.Vars.diff_median | printf "%.2f"}}
    <br>Calculated Threshold is: {{.Eval .Alert.Vars.past | printf "%.2f"}}
    <br>Histroical Median: {{.Eval .Alert.Vars.past_median | printf "%.2f"}}
    {{printf "q(\"avg:rate{counter,,1}:os.cpu{host=%s}\", \"1d\", \"\")" (.Group.host) | .Graph}}
    `
    subject = `
    {{ $anon := gt (.Eval .Alert.Vars.warn_anon) 0.0}}
    {{ $thresh := or (gt (.Eval .Alert.Vars.warn_thresh) 0.0) (gt (.Eval .Alert.Vars.crit_thresh) 0.0)}}
    {{.Last.Status}}: CPU on {{.Group.host}} due to {{if $anon }}{{.Eval .Alert.Vars.diff_median | printf "%.2f"}}% greater utilization than the historical median{{end}}{{if and $thresh $anon}} and {{end}}{{if $thresh}}high cpu {{.Eval .Alert.Vars.recent_avg | printf "%.2f"}}% {{end}}`
}

template netbackup {
    subject = `{{.Last.Status}}: Netbackup has {{.Eval .Alert.Vars.summary}} Problematic Backups`
    body = `
            {{template "header" .}}
            <p><a href="http://www.symantec.com/business/support/index?page=content&id=DOC5181">Symantec Reference Guide for Status Codes (PDF)</a>
            <p><a href="https://ny-back02.ds.stackexchange.com/opscenter/">View Backups in Opscenter</a>
            <h3>Status of all Tape Backups</h3>
        <table>
        <tr><th>Client</th><th>Policy</th><th>Schedule</th><th>Frequency</th><th>Attempt Age</th><th>Status</th></tr>
    {{ range $v := .LeftJoin .Alert.Vars.job_frequency .Alert.Vars.attempt_age .Alert.Vars.job_status}}
        {{ $freq := index $v 0 }}
        {{ $age := index $v 1 }}
        {{ $status := index $v 2 }}
        <tr>
            <td><a href="https://status.stackexchange.com/dashboard/node?node={{$freq.Group.client| short}}">{{$freq.Group.client| short}}</td>
            <td>{{$freq.Group.class}}</td>
            <td>{{$freq.Group.schedule}}</td>
            <td>{{$freq.Value}}</td>
            <td {{if gt $age.Value $freq.Value }} style="color: red;" {{end}}>{{$age.Value | printf "%.2f"}}</td>
            <td{{if gt $status.Value 0.0}} style="color: red;" {{end}}>{{$status.Value}}</td>
        <tr>
    {{end}}
    </table>`
}

template linux.bonding {
    subject = {{.Last.Status}}: {{.Eval .Alert.Vars.by_host}} bad bond(s) on {{.Group.host}}
    body = `{{template "header" .}}
    <h2>Bond Status</h2>
    <table>
    <tr><th>Bond</th><th>Slave</th><th>Status</th></tr>
    {{range $r := .EvalAll .Alert.Vars.slave_status}}
        {{if eq $.Group.host .Group.host}}
            <tr>
                <td>{{$r.Group.bond}}</td>
                <td>{{$r.Group.slave}}</td>
                <td {{if lt $r.Value 1.0}} style="color: red;" {{end}}>{{$r.Value}}</td>
            </tr>
        {{end}}
    {{end}}
    </table>
    `
}

template it.site.down {
    subject = {{.Last.Status}}: POTENTIAL SITE OUTAGE: {{.Group.destination}} has {{.Eval  .Alert.Vars.q}}% summed packet loss
    body = `
    {{template "header" .}}
    {{.Graph .Alert.Vars.series}}
    {{template "def" .}}
    {{template "tags" .}}
    {{template "computation" .}}
    `
}

template ping.location {
    body = `{{template "header" .}}
    <h3>Reachability Summary for {{ .Alert.Vars.name }}</h3> 
    <table>
        <tr><td>Percent of Hosts Timing Out</td><td>{{.Eval .Alert.Vars.percent_timeout | printf "%.2f"}}%</td></tr>
        <tr><td>Total Hosts</td><td>{{.Eval .Alert.Vars.total_hosts}}</td></tr>
        <tr><td>Hosts Timing Out</td><td>{{.Eval .Alert.Vars.hosts_timing_out}}</td></tr>
    </table>
    <h3>Hosts Timing Out</h3>
    {{ range $r := .EvalAll .Alert.Vars.pq }}
            {{if gt $r.Value 0.0}}
                {{$r.Group.dst_host}}</br>
            {{end}}
        {{end}}
    </br>
    {{template "def" .}}`
    subject = {{.Last.Status}}: Packet Loss to {{.Alert.Vars.name}}: {{.Eval .Alert.Vars.percent_timeout | printf "%.2f" }}% ({{.Eval .Alert.Vars.hosts_timing_out}}/{{.Eval .Alert.Vars.total_hosts}})
}

template elastic.cluster.status {
    body = `{{template "header" .}}
    <p><a href="https://status.stackexchange.com/elastic">View Elastic Status in Opserver</a>

    `
    subject = `{{.Last.Status}}: Elastic Cluster {{.Group.cluster}} status is {{if gt (.Eval .Alert.Vars.status) 0.0}}Bad{{else}}Good{{end}}`
}


###################
## Notifications
###################

notification default {
    #post = http://chat.meta.stackoverflow.com/feeds/rooms/704?key=F5B94705-C34C-403F-A7EF-0362163AB67B
    email = sre-alerts@stackoverflow.com
}

notification it-alerts {
    email = it-alerts@stackexchange.com
}

notification it-bonfire-chat {
	post = http://chat.meta.stackexchange.com/feeds/rooms/602?key=eaa2c88f-1121-44bf-860a-6e2a3ddfce71
}

notification bosun {
    email = mjibson@stackoverflow.com, kyle@stackoverflow.com
}

notification kyle {
    email = kyle@stackoverflow.com
}

notification tom {
    email = tlimoncelli@stackoverflow.com
}

notification production {
    email = production-team@stackoverflow.com
}

notification core {
    email=core@stackoverflow.com
}

notification careers {
    email=careers-devs@stackoverflow.com
}

notification mobile {
    email=mobile@stackoverflow.com
}


###################
## Global Alert Variables
###################
$disk_squelch = disk=/media.*|/home/gbeech.*|/tmp/.*|/dev/shm

###################
## Alerts
###################

$default_time = "5m"

lookup host_base_contact {
    entry host=nyhq-|-int|den-*|lon-* {
        main_contact = it-alerts
        chat_contact = it-bonfire-chat
    }
    entry host=* {
        main_contact = default
    }
}

macro host_based {
    warnNotification = lookup("host_base_contact", "main_contact")
    critNotification = lookup("host_base_contact", "main_contact")
    warnNotification = lookup("host_base_contact", "chat_contact")
    critNotification = lookup("host_base_contact", "chat_contact")
}

alert os.low.memory {
    macro = host_based
    template = generic
    $notes = In Linux, Buffers and Cache are considered "Free Memory"
    $unit_string = % Free Memory
    $q = avg(q("avg:os.mem.percent_free{host=*}", $default_time, ""))
    crit = $q <= .5
    warn = $q < 5
    squelch = host=sql|devsearch
}

lookup host_drops {
    entry host=ny-omni01 {
        threshold  = 5000
    }
    entry host=* {
        threshold = 200
    }
}

alert os.net.drop_err {
    $notes = The threshold is per-interface, but the alert trigger on a host scope. So if any interface on the host exceeds the drop threshold then the alert is triggered. This is done because often drops happen across multiple interfaces on the same host, and by having only one alert notification noise is reduced.
    macro = host_based
    template = interface_drops
    $ds = 2m-max
    $iface_drop = avg(q("avg:$ds:rate{counter,,1}:os.net.dropped{host=*,iface=*}", "10m", ""))
    $iface_err = avg(q("avg:$ds:rate{counter,,1}:os.net.errs{host=*,iface=*}", "10m", ""))
    $iface_drop_err = $iface_drop + $iface_err
    $iface_exceeds_threshold = t($iface_drop_err, "host") > lookup("host_drops", "threshold")
    $exceed_count = sum($iface_exceeds_threshold)
    warn = $exceed_count
    squelch = host=ny-omni01
}

lookup disk_space {
    entry host=ny-omni01,disk=E {
        #Setting -1 disables the forecast attribute
        warn_days = -1
        crit_days = -1
        warn_percent_free = .01
        crit_percent_free = 0
    }
    entry host=or-sql*,disk=E {
        warn_days = -1
        crit_days = -1
        warn_percent_free = 4
        crit_percent_free = 0
    }
    entry host=ny-db05,disk=* {
        warn_days = -1
        crit_days = -1
        warn_percent_free = .01
        crit_percent_free = 0
    }
    entry host=*,disk=* {
        warn_days = 7
        crit_days = 1
        warn_percent_free = 10
        crit_percent_free = 0
    }
}

alert os.diskspace {
    macro = host_based
    $notes = This alert triggers when there are issues detected in disk capacity. Two methods are used. The first is a traditional percentage based threshold. This alert also uses a linear regression to attempt to predict the amount of time until we run out of space. Forecasting is a hard problem, in particular to apply generically so there is a lot of room for improvement here. But this is a start
    template = diskspace
    $filter = host=*,disk=*
    
    ##Forecast Section
    #Downsampling avg on opentsdb side will save the linear regression a lot of work
    $days_to_zero = (forecastlr(q("avg:6h-avg:os.disk.fs.percent_free{$filter}", "7d", ""), 0) / d("1d"))
    #Threshold can be higher here once we support string lookups in lookup tables https://github.com/bosun-monitor/bosun/issues/268
    $warn_days = $days_to_zero > 0 && $days_to_zero < lookup("disk_space", "warn_days")
    $crit_days =   $days_to_zero > 0 && $days_to_zero < lookup("disk_space", "crit_days")
    
    ##Percent Free Section
    $pf_time = "5m"
    $percent_free = avg(q("avg:os.disk.fs.percent_free{host=*,disk=*}", $pf_time, ""))
    $used = avg(q("avg:os.disk.fs.space_used{host=*,disk=*}", $pf_time, ""))
    $total = avg(q("avg:os.disk.fs.space_total{host=*,disk=*}", $pf_time, ""))
    $free = $total - $used
    $warn_percent = $percent_free <  lookup("disk_space", "warn_percent_free")
    #Linux stops root from writing at less than 5%
    $crit_percent = $percent_free <  lookup("disk_space", "crit_percent_free")
    #For graph (long time)
    $percent_free_graph = q("avg:1h-min:os.disk.fs.percent_free{host=*,disk=*}", "4d", "")
    
    ##Main Logic
    warn = $warn_percent || $warn_days
    crit = $crit_percent || $crit_days
    
    ##Options
    squelch = $disk_squelch
    ignoreUnknown = true
    #This is needed because disks go away when the forecast doesn't
    unjoinedOk = true
    
}

alert redis.fragmentation {
    macro = host_based
    template = redis.fragmentation
    $notes = memory fragementation is how much memory redis needs vs how much physical ram it is taking. It ends up taking more physical memory due to fragmentation. The alert spans a longer period of time (30 minutes) because when redis is reslaving or restarting sometimes there are spikes in fragmentation that we are not interested in.
    $redis_used = median(q("sum:redis.used_memory{host=*,port=*}", "1h", ""))
    $used_rss = median(q("sum:redis.used_memory_rss{host=*,port=*}", "1h", ""))
    $frag = median(q("sum:redis.mem_fragmentation_ratio{host=*,port=*}", "1h", ""))
    warn = $frag > 2 && $used_rss > 2e9
    crit = $frag > 5 && $used_rss > 2e9
    squelch = host=.*-devredis.*
}

alert haproxy.anomalous.sessions {
    template = generic
    $metric = "sum:rate{counter,,1}:haproxy.frontend.stot"
    $duration = "30m"
    $period = "7d"
    $lookback = 4
    $history = band($metric, $duration, $period, $lookback)
    $past_dev = dev($history)
    $past_avg = avg($history)
    $current_avg = avg(q($metric, $duration, ""))
    warn = $current_avg > $past_avg + $past_dev*8 || $current_avg < $past_avg - $past_dev*8
    warnNotification = default
}

alert haproxy.server.error_connections {
    template = generic
    $metric = "sum:rate{counter,,1}:haproxy.server.econ"
    $q = sum(q($metric, $default_time, ""))
    warn = $q > 0
    warnNotification = default
}

alert haproxy.server.downtime {
    macro = host_based
    template = haproxy.server.downtime
    $notes = This alert triggers when any server (as haproxy defines a server) has been down for more than 2 of the last 5 minutes and that server is not in maintenance.  The notification is scoped to our entire haproxy service. The consequence of that is that if 1 server goes down somewhere after one is down we won't get another notification. However, there will be another critical notification if there are more than 50% of servers down on any unique Host,Backend,Server combination. Because of this, it is important to be disciplined in handling this alert.
    $time = "5m"
    $threshold = 120.0
    $metric = "sum:rate{counter,,1}:haproxy.server.downtime{svname=*,pxname=*,host=*}"
    $maint_metric = "max:haproxy.server.status{svname=*,pxname=*,host=*}"
    $downtime = change($metric, $time, "")
    $maintenance = max(q($maint_metric, $time, ""))
    # Breakdown by servers down per backend per host
    $svcount = len(t($downtime, "host,pxname"))
    $svdowncount = sum(t($downtime > $threshold && max(q($maint_metric, $time, "")) != 3, "host,pxname"))
    $percent_down = $svdowncount / $svcount * 100
    $max_percent_down = max(t($percent_down, ""))
    $max_down_count = max(t($svdowncount, ""))
    warn = $max_down_count
    crit =  $max_percent_down > 50
    squelch = host=ny-lb03|ny-lb04
}

alert lb.ip_count_changed {
    macro = host_based
    template = generic
    $q = diff("sum:linux.net.ip_count{version=4,host=*-lb*}", "5m", "")
    crit = $q
    critNotification = default
}

lookup redis {
    entry port=6379 {
        slave_count=3
        name=Core (Q&A)
        notify=core
        }
    entry port=6380 {
        slave_count=3
        name=Careers
        notify=careers
    }
    entry port=6381 {
        slave_count=3
        name=Gossip (Q&A)
        notify=core
    }
    entry port=6382 {
        slave_count=3
        name=Analytics (Careers)
        notify=careers
    }
    entry port=6383 {
        slave_count=3
        name=CoreML (Q&A)
        notify=core
    }
    entry port=6384 {
        slave_count=3
        name=Mobile
        notify=mobile
    }
    entry port=6385 {
        slave_count=3
        name=Ads
        #Need Mailing list for notify
    }
    entry port=6387 {
        slave_count=3
        name=PRZIM
        notify=core
    }
    entry port=6388 {
        slave_count=2
        name=Per-DC
        notify=core
    }
    entry port=* {
        name=Unknown Instance
    }
}

alert redis.replication {
    template = redis.replication
    $notes = This alert determines if replication is working by counting the number of slaves. It counts the number of slaves by finding out the master_link_status and summing that status (1 for working) to get the expected number of slaves. So redis instance is either not a slave, or the slave is not syncing properly then master_link_status will not be 1.  Some of redis instances expect there to be 2 masters and 2 slaves, others expect 1 master and 3 slaves.   We also trigger if there are more than 1 connected slaves on any single instance since we expect replication to always take place in series (not a star topology)
    $time = "15m"
    $link_status = q("sum:redis.master_link_status{host=*-redis*,port=*}", $time, "")
    $is_slave = min(q("sum:redis.is_slave{host=*-redis*,port=*}", $time, ""))
    $sync_status = min(q("sum:redis.master_sync_in_progress{host=*-redis*,port=*}", $time, ""))
    $sync_ago = last(q("sum:redis.master_last_io_seconds_ago{host=*-redis*,port=*}", $time, ""))
    
    $connected_slaves = last(q("sum:redis.connected_slaves{host=*-redis*,port=*}", $time, ""))
    $too_many_slaves = max(t($connected_slaves, "port") > 1)
    $link_status_by_port = sum(t(min($link_status), "port"))
    $health_by_port = $link_status_by_port != lookup("redis", "slave_count") || $too_many_slaves
    $total_bad = sum(t($health_by_port, ""))
    crit = $total_bad
    critNotification = default
    #Currently can't do the below because the lookup is constrained to the alert scope
    #critNotification = lookup("redis", "notify")
    squelch = port=6378
}

alert high_cpu {
    macro = host_based
    template = high.cpu
    #squelch CPU on ML machines until there is some sort of normal condition
    squelch = host=nyhq-intmon01|den-intmon01|lon-intmon01|ny-ml0
    $metric = "avg:rate{counter,,1}:os.cpu{host=*}"
    
    #Anomaolous Detection
    $duration = "1h"
    $period = "7d"
    $lookback = 4
    $history = band($metric, $duration, $period, $lookback)
    $past_dev = dev($history)
    $past_median = median($history)
    $current_median = median(q($metric, $duration, ""))
    $diff_median = $current_median - $past_median
    $past = $past_median + $past_dev*3
    $warn_anon = $current_median > $past && $current_median > 20 && $diff_median > 10
    
    #Threshold Detection
    $recent_avg = avg(q($metric, "10m", ""))
    $hour_avg = avg(q($metric, $duration, ""))
    $warn_thresh = $hour_avg > 90
    $crit_thresh = $recent_avg >= 99 && $recent_avg < 110
    
    #Graph
    $cpu_graph = q($metric, "1d", "")
    
    warn = ($warn_anon) || ($warn_thresh)
    crit = $crit_thresh
}

alert linux.high_cpu_per_core {
    macro = host_based
    template = generic
    $notes = This takes all the cores per host, and alerts if anyone of those cores has been above 95% utilization for the past 50 minutes. This alert can be incorporated to the general cpu alert once we update the os namespace to include a per_core breakdown
    #Used is 100-idle
    $per_core = min(100-q("sum:10m-min:rate{counter,,1}:linux.cpu.percpu{cpu=*,host=*,type=idle}", "50m", ""))
    $t_core = t($per_core, "host")
    $max_core = max($t_core)
    $core_count = len($t_core)
    $q = $max_core
    #Don't alert if only one core, because the normal cpu alert should cover that case
    warn = $max_core > 95 && $core_count != 1
}

alert linux.swapping {
    macro = host_based
    template = generic
    $notes = This alert does not trigger for our mail servers with the mail queue is high
    $mail_q = nv(max(q("sum:exim.mailq_count{host=*}", "2h", "") > 5000), 1)
    $metric = "sum:rate{counter,,1}:linux.mem.pswp{host=*,direction=in}"
    $q =  (median(q($metric, "2h", "")) > 1)
    $trigger = $q && ! $mail_q
    warn = $trigger
    squelch = host=ny-devsearch*|ny-git01
}

alert linux.entropy {
    macro = host_based
    template = generic
    $metric = "min:linux.entropy_avail{host=*}"
    $q = percentile(q($metric, "10m", ""), 0)
    warn = $q <= 0
}

alert linux.net.conntrack.percent.used {
    macro = host_based
    template = generic
    $metric = "sum:linux.net.conntrack.percent_used{host=*}"
    $q = percentile(q($metric, "10m", ""), 100)
    warn = $q > 80
    crit = $q >= 99
}

lookup linux_tcp {
    entry host=ny-tsdb03 {
        backlog_drop_threshold = 500
    }
    entry host=* {
        backlog_drop_threshold = 300
    }
}

alert linux.tcp {
    macro = host_based
    $notes = This alert checks for various errors or possible issues in the TCP stack of a Linux host. Since these tend to happen at the same time, combining them into a single alert reduces notification noise.
    template = linux.tcp
    $time = 10m
    $abort_failed = change("sum:rate{counter,,1}:linux.net.stat.tcp.abortfailed{host=*}", "$time", "")
    $abort_mem = change("sum:rate{counter,,1}:linux.net.stat.tcp.abortonmemory{host=*}", "$time", "")
    $ofo_pruned = change("sum:rate{counter,,1}:linux.net.stat.tcp.ofopruned{host=*}", "$time", "")
    $rcv_pruned = change("sum:rate{counter,,1}:linux.net.stat.tcp.rcvpruned{host=*}", "$time", "")
    $backlog_drop = change("sum:rate{counter,,1}:linux.net.stat.tcp.backlogdrop{host=*}", "$time", "")
    $syncookies_sent = change("sum:rate{counter,,1}:linux.net.stat.tcp.syncookiessent{host=*}", "$time", "")
    $total_err = $abort_failed + $ofo_pruned + $rcv_pruned + $backlog_drop + $syncookies_sent
    warn = $abort_failed || $ofo_pruned > 100 || $rcv_pruned > 100 || $backlog_drop > lookup("linux_tcp", "backlog_drop_threshold")  || $syncookies_sent
}

alert linux.disk.time_per_write.95th {
    macro = host_based
    template = generic
    $oquery = "sum:rate{counter,,1}:linux.disk.time_per_write{host=*,dev=*}"
    $q = percentile(q($oquery, "10m", ""), .95)
    warn = $q > 10
    warnNotification = default
    ignoreUnknown = true
    squelch = $disk_squelch
}

alert linux.disk.time_per_read.95th {
    macro = host_based
    template = generic
    $oquery = "sum:rate{counter,,1}:linux.disk.time_per_read{host=*,dev=*}"
    $q = percentile(q($oquery, "10m", ""), .95)
    warn = $q > 10
    warnNotification = default
    squelch = $disk_squelch
}

alert elastic.cluster.status {
    template =  elastic.cluster.status
    $metric = "sum:elastic.cluster.status{cluster=*}"
    $status = max(q($metric, "10m", ""))
    crit = $status
    critNotification = default
    squelch = cluster=StackExchangeNetwork-Dev
}

#alert elastic.node.unbalanced {
#    template = generic
#    $t = "5m"
#    $cluster_tag=cluster
#    $node_tag=host
#    $metric=elastic.search.query_total
#    $cluster_metric = "sum:rate{counter,,1}:$metric{$cluster_tag=*}"
#    $node_metric = "sum:rate{counter,,1}:$metric{$cluster_tag=*,$node_tag=*}"
#    $cluster_total = avg(q($cluster_metric, $t, ""))
#    $node_total = avg(q($node_metric, $t, ""))
#    $node_perc = 1/len(t(avg(q($node_metric, $t, "")), "cluster"))
#    $q = abs($node_perc - ($node_total / $cluster_total)) > $node_perc * .5
#    warn = $q
#}

alert asterisk.call.quality {
    template = asterisk.call.quality
    $oquery = "sum:asterisk.mos_lqo{sender=7203624531|6466017997,receiver=7203624531|6466017997}"
    $q = avg(q($oquery, "12m", ""))
    #Lower numbers are worse call quality
    warn = $q < 3.25
    crit = $q < 3.0
    warnNotification = it-alerts,it-bonfire-chat
    critNotification = it-alerts,it-bonfire-chat
}

alert puppet.left.disabled {
    macro = host_based
    template = generic
    $notes = More often than not, if puppet has been consistently disabled for more than 24 hours some forgot to re-enable it
    $oquery = "avg:24h-min:puppet.disabled{host=*}"
    $q = min(q($oquery, "24h", ""))
    warn = $q > 0
}

alert puppet.failed_resources {
    macro = host_based
    template = puppet.failed_resources
    $t = 2h
    $notes = This alert will trigger when a host has consistently had failed resources for longer than $t
    $q = min(q("sum:1h-min:puppet.run.resources{resource=failed,host=*}", "$t", ""))
    warn = $q
}

# Detect an increase in MY-SQL Slow Queries.
alert mysql.slow_queries {
    macro = host_based
    template = generic.cw
    $q = max(q("sum:5m-avg:rate{counter,,1}:mysql.slow_queries{host=*}", "5m", ""))
    crit = $q > 30
    warn = $q > 10
}

alert haproxylogs.server_err.percent {
    template = generic
    $err = ungroup(avg(q("avg:rate{counter,,1}:haproxy.logs.response{type=server_err}", "5m", ""))) 
    $success = ungroup(avg(q("avg:rate{counter,,1}:haproxy.logs.response{type=success}", "5m", "")))
    $q = $err / ($err + $success) * 100 > 1
    warn = $q > 1
    warnNotification = default
}

alert hardware {
    macro = host_based
    template = hardware
    $time = "30m"
    
    #By Component
    $power = max(q("sum:hw.ps{host=*,id=*}", $time, ""))
    $battery = max(q("sum:hw.storage.battery{host=*,id=*}", $time, ""))
    $controller = max(q("sum:hw.storage.controller{host=*,id=*}", $time, ""))
    $enclosure = max(q("sum:hw.storage.enclosure{host=*,id=*}", $time, ""))
    $physical_disk = max(q("sum:hw.storage.pdisk{host=*,id=*}", $time, ""))
    $virtual_disk = max(q("sum:hw.storage.vdisk{host=*,id=*}", $time, ""))
    #I believe the system should report the status of non-zero if the anything is bad
    #(omreport system), so everything else is for notification purposes. This works out
    # because not everything has all these components
    $system = max(q("sum:hw.system{host=*,component=*}", $time, ""))
    
    #Component Summary Per Host
    $s_power= sum(t($power, "host"))
    $s_battery = sum(t($battery, "host"))
    
    warn = $system
    squelch = host=ny-devsql01
}

alert asterisk.office_numbers.usa_toll_free {
    template = asterisk.call.quality
    $t = "1d"
    $q = avg(q("avg:asterisk.mos_lqo{receiver=2122328294,sender=*}", $t, ""))
    warn = $q < 3.25
    crit = $q < 3.0
    warnNotification = it-alerts,it-bonfire-chat
    critNotification = it-alerts,it-bonfire-chat
}

alert asterisk.office_numbers.nyhq_main_line {
    template = asterisk.call.quality
    $t = "1d"
    $q = avg(q("avg:asterisk.mos_lqo{receiver=2122328280,sender=*}", $t, ""))
    warn = $q < 3.25
    crit = $q < 3.0
    warnNotification = it-alerts,it-bonfire-chat
    critNotification = it-alerts,it-bonfire-chat
}

alert asterisk.office_numbers.denver_main_line {
    template = asterisk.call.quality
    $t = "1d"
    $q = avg(q("avg:asterisk.mos_lqo{receiver=3034466500,sender=*}", $t, ""))
    warn = $q < 3.25
    crit = $q < 3.0
    warnNotification = it-alerts,it-bonfire-chat
    critNotification = it-alerts,it-bonfire-chat
}

alert asterisk.office_numbers.uk_test_number {
    template = asterisk.call.quality
    $t = "1d"
    $q = avg(q("avg:asterisk.mos_lqo{receiver=4250,sender=17203624531}", $t, ""))
    warn = $q < 3
    crit = $q < 2
    warnNotification = it-alerts,it-bonfire-chat
    critNotification = it-alerts,it-bonfire-chat
}

alert opentsdb.relay {
    template = generic
    $q = avg(q("sum:rate{counter,,1}:bosun.relay.do_err{host=*}", "5m", ""))
    crit = $q > 0
    critNotification = bosun
}

alert railgun_down {
    template = generic
    $q = avg(q("avg:rate{counter,,1}:railgun.time{host=ny-lb03|ny-lb04|ny-lb05|ny-lb06}", "5m", ""))
    crit = $q == 0
    critNotification = tom
    squelch = host=ny-railgun*
}

alert hbase.replication_queue {
    macro = host_based
    template = generic
    $q = avg(q("sum:530s-avg:hbase.region.sizeOfLogQueue{instance=*,host=*}", "30m", ""))
    crit = $q > 10
}

alert ntp.high_offset {
    macro = host_based
    template = generic
    $notes = 3 Seconds is when keepalive breaks
    $offset = max(q("avg:ntp.offset{host=*,remote=*}", "5m", ""))
    $current = max(q("avg:ntp.current_source{host=*,remote=*}", "5m", "")) == 1
    warn = abs($offset) > 1000 && $current
    crit = abs($offset) > 3000 && $current
}

alert anomalous.temperature {
    macro = host_based
    template = generic.it
    $metric = "avg:environment.temperature{host=den-ups01|lon-ups01|nyhq-ups01|nyhq-ups02}"
    $duration = "30m"
    $period = "1d"
    $lookback = 5
    $history = band($metric, $duration, $period, $lookback)
    $past_dev = dev($history)
    $past_avg = avg($history)
    $q = avg(q($metric, $duration, ""))
    $pastwarn_high = $past_avg + $past_dev+5
    $pastcrit_high = $past_avg + $past_dev+10
    $pastwarn_low = $past_avg - $past_dev-5
    $pastcrit_low = $past_avg - $past_dev-10
    warn = ($q > $pastwarn_high) || ($q < $pastwarn_low)
    crit = ($q > $pastcrit_high) || ($q < $pastcrit_low)
}

alert it.ping.packetloss {
    macro = host_based
    template = it.packetloss
    $metric = "sum:net.ping.packetloss{destination=*,host=*}"
    $duration = "1h"
    $q = last(q($metric,$duration,""))
    warn = $q > 20
    crit = $q > 30
    #Squelch OR since I want to use a different alert of this with SRE
    squelch = destination=or-*
}

#This needs more tuning, but commiting it before I lose it
alert slower.route.performance {
    template = route.performance
    $notes = Response time is based on HAProxy's Tr Value. This is the web server response time (time elapsed between the moment the TCP connection was established to the web server and the moment it send its complete response header
    $duration = "1d"
    $route=*
    $metric = "sum:10m-avg:haproxy.logs.route_tr_median{route=$route}"
    $route_hit_metric = "sum:10m-avg:rate{counter,,1}:haproxy.logs.hits_by_route{route=$route}"
    $total_hit_metric = "sum:10m-avg:rate{counter,,1}:haproxy.logs.hits_by_route"
    $route_hits = change($route_hit_metric, $duration, "")
    $total_hits = change($total_hit_metric, $duration, "")
    $hit_percent = $route_hits / $total_hits * 100
    $current_hitcount = len(q($metric, $duration, ""))
    $period = "7d"
    $lookback = 4
    $history = band($metric, $duration, $period, $lookback)
    $past_dev = dev($history)
    $past_median = percentile($history, .5)
    $current_median = percentile(q($metric, $duration, ""), .5)
    $diff = $current_median - $past_median
    warn = $current_median > ($past_median + $past_dev*2) && abs($diff) > 10 && $hit_percent > 1
    warnNotification = default
    ignoreUnknown = true
}

alert exceptions {
    template = exceptions
    $notes = This alert monitors exceptions from exceptional and triggers when there are large number of application level exceptions. 
    $metric = sum:rate{counter,,1}:exceptional.exceptions.count
    $t = 10m
    $threshold = 1000
    $graph_time = 2h
    $exceptions = q("$metric", "$t", "")
    $exceptions_total = change("$metric", "$t", "")
    $exceptions_graph = q("$metric", "$graph_time", "")
    $avg_exceptions =  avg($exceptions)
    $machine = change("$metric{machine=*}", "$t", "")
    $machine_graph = q("$metric{machine=*}", "$graph_time", "")
    $application = change("$metric{application=*}", "$t", "")
    $application_graph = q("$metric{application=*}", "$graph_time", "")
    warn = $exceptions_total > $threshold
    warnNotification = production
}

alert haproxy_session_limit {
    macro = host_based
    template = generic
    $notes = This alert monitors the percentage of sessions against the session limit in haproxy (maxconn) and alerts when we are getting close to that limit and will need to raise that limit. This alert was created due to a socket outage we experienced for that reason
    $current_sessions = max(q("sum:haproxy.frontend.scur{host=*,pxname=*,tier=*}", "5m", ""))
    $session_limit = max(q("sum:haproxy.frontend.slim{host=*,pxname=*,tier=*}", "5m", ""))
    $q = ($current_sessions / $session_limit) * 100
    warn = $q > 80
    crit = $q > 95
}

alert bosun_build {
    template = bosun_build
    $notes = This alerts when a bosun build has failed in the last 5 minutes
    $build = min(q("sum:tc.build_success{build_type=*NewYork_BosunProd*|*NewYork_BosunDev*}", "5m", ""))
    warn = ! $build 
    warnNotification = bosun
}

alert route.anomalous.hits {
    template = generic
    ignoreUnknown = true
    $metric = "sum:rate{counter,,1}:haproxy.logs.hits_by_route{route=*}"
    $duration = "30m"
    $period = "7d"
    $lookback = 4
    $history = band($metric, $duration, $period, $lookback)
    $past_dev = dev($history)
    $past_avg = avg($history)
    $current_avg = avg(q($metric, $duration, ""))
    warn = ($current_avg > $past_avg + $past_dev*6 || $current_avg < $past_avg - $past_dev*6) && $current_avg > 30
    warnNotification = kyle
}

alert netbackup {
    template = netbackup
    $tagset = {class=*,client=*,schedule=*}
    $attempt_age = max(q("sum:netbackup.backup.attempt_age$tagset", "10m", "")) / d("1d")
    $job_frequency = max(q("sum:netbackup.backup.frequency$tagset", "10m", "")) / d("1d")
    $job_status = max(q("sum:netbackup.backup.status$tagset", "10m", ""))
    #Add 1/4 a day to the frequency as some buffer
    $not_run_in_time = nv($attempt_age, 1e9) > nv($job_frequency+.25, 1)
    $problems = $not_run_in_time || nv($job_status, 1)
    $summary = sum(t($problems, ""))
    warn = $summary
    warnNotification = kyle
}

alert linux.bonding {
    template = linux.bonding
    macro = host_based
    $notes = This alert triggers when a bond only has a single interface, or the status of a slave in the bond is not up
    $slave_status = max(q("sum:linux.net.bond.slave.is_up{bond=*,host=*,slave=*}", "5m", ""))
    $slave_status_by_bond = sum(t($slave_status, "host,bond"))
    $slave_count = max(q("sum:linux.net.bond.slave.count{bond=*,host=*}", "5m", ""))
    $no_good = $slave_status_by_bond < $slave_count || $slave_count < 2
    $by_host = max(t($no_good, "host"))
    warn = $by_host
}

alert it.logstash.output.rate {
    macro=host_based
    template = generic.it
    $metric= "sum:logstash.output_rate{host=*}"
    $duration = "5m"
    $hysteresis_percent=10
    $high_watermark=150000
    $low_watermark=2000
    $period="1w"
    $lookback=2
    $history= band($metric,$duration, $period, $lookback)
    $past_dev = dev($history)
    $past_avg = avg($history)
    $q = avg(q($metric, $duration, ""))

    $pastwarn_high = $past_avg + ($past_dev+($past_dev*($hysteresis_percent/100)))
    $pastwarn_low = $past_avg - ($past_dev+($past_dev*($hysteresis_percent/100)))
    crit = ($q > $high_watermark) || ($q < $low_watermark)
    #History is not fully formed yet, so, we'll let warn and crit share conditions for now.
    warn = ($q > $high_watermark) || ($q < $low_watermark)
    #warn = ($q > $pastwarn_high) || ($q < $pastwarn_low)
}

alert it.site.down {
    macro=host_based
    template = it.site.down
    $duration="1m"
    $metric="sum:60s-avg:net.ping.packetloss{destination=*}"
    $series=q($metric, $duration, "")
    $q=avg($series)
    warnNotification = it-alerts,it-bonfire-chat
    critNotification = it-alerts,it-bonfire-chat
    ignoreUnknown = true
    warn = $q > 100
    crit = $q > 150
    #Squelch OR since I want to use a different alert of this with SRE
    squelch = destination=or-*
}

macro ping_location {
    template = ping.location
    $pq = max(q("sum:bosun.ping.timeout{dst_host=$loc*,host=$source}", "5m", ""))
    $grouped = t($pq,"")
    $hosts_timing_out = sum($grouped)
    $total_hosts = len($grouped)
    $percent_timeout = $hosts_timing_out / $total_hosts * 100
    crit = $percent_timeout > 10
}

alert or_hosts_down {
    $source=ny-bosun01
    $loc = or-
    $name = OR Peak
    macro = ping_location
    critNotification = default
}